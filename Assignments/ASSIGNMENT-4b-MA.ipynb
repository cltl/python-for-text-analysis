{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 MA Part B\n",
    "\n",
    "**<mark>Attention: This assignment may still be revised. Please download it again at the beginning of Block IV. </mark>**\n",
    "\n",
    "**Deadline for Assignment 4a+b: Friday October 14, 2022, before 17:00 via Canvas (Assignment 4)** \n",
    "\n",
    "Tips and clarifications may be posted here: https://docs.google.com/document/d/1sgfaMqJX6raXPiG2Ffk0Hbi1_FPJtbTjd31E9LQBP0E/edit?usp=sharing\n",
    "\n",
    "## A corpus of TED talks and their translations\n",
    "\n",
    "In this assignment, you are going to work with a corpus of transcribed TED talks (English) and their translations into other languages. The resource was developed by \n",
    "\n",
    "\n",
    "The resource is called WIT3 - acronym for Web Inventory of Transcribed and Translated Talks.\n",
    "\n",
    "\n",
    "In part I of the assignment, you are going to analyze the original talks. In part II, you are going to explore the translations. \n",
    "\n",
    "The learning goals of this assginment are the following:\n",
    "\n",
    "* Become comfortable with extracting information from xml\n",
    "* Learn how to explore a corpus consisting of multiple files\n",
    "* Lean how to map information accross mutliple files\n",
    "* Take the first steps in indepdently structuring code\n",
    "\n",
    "\n",
    "If you are interesed in learning more about the resource and what it is used for, you can check the following paper:\n",
    "\n",
    "\n",
    "M. Cettolo, C. Girardi, and M. Federico. 2012. WIT3: Web Inventory of Transcribed and Translated Talks.\n",
    "In Proc. of EAMT, pp. 261-268, Trento, Italy [pdf].\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Prepare the data and understand the corpus structure \n",
    "\n",
    "**1.) Download**\n",
    "\n",
    "Please go to the following url: https://wit3.fbk.eu/home\n",
    "\n",
    "Download the latest version of the corpus by clicking on \"Talks in XML format (109 languages)\". \n",
    "\n",
    "Please the download in the Data directory (`../Data/`). Please create a new folder called `ted-talks` and move the download there. Unpack it (should work by double-clicking on it). \n",
    "\n",
    "\n",
    "The corpus you downloaded contains multiple releases of the data. You data should be at `../Data/ted-talks/XML_releases/`.\n",
    "\n",
    "\n",
    "Please run the following cell below to check. \n",
    "\n",
    "You should see these files and subdirectories:\n",
    "\n",
    "* tools.html \n",
    "* wit3_xml-20140120.zip \n",
    "* wit3.dtd               \n",
    "* xml/                   \n",
    "* xml-20150616/\n",
    "* xml-20140120/\n",
    "\n",
    "The directories starting with 'xml' contain different versions of the data. We will focus on the most recent version of the data in the directory `xml/`. Feel free to remove the remaining two directories. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tools.html             wit3_xml-20140120.zip  \u001b[1m\u001b[34mxml-20140120\u001b[m\u001b[m/\n",
      "wit3.dtd               \u001b[1m\u001b[34mxml\u001b[m\u001b[m/                   \u001b[1m\u001b[34mxml-20150616\u001b[m\u001b[m/\n"
     ]
    }
   ],
   "source": [
    "%ls ../Data/ted-talks/XML_releases/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.) Unzipping the xml files**\n",
    "\n",
    "If you run the next cell, you will see that the xml files are still zipped.\n",
    "\n",
    "To unzip all files at once, navigate to the directory on your command line and run: `unzip \"*.zip\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls ../Data/ted-talks/XML_releases/xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.) Understanding the structure of the corpus** \n",
    "\n",
    "\n",
    "List only the xml files in the corpus using the cell below. \n",
    "\n",
    "You will see that all file names follow the same convention: ted-[language]-[release-date].xml\n",
    "\n",
    "\n",
    "Each file contains all talks translated to the target language (i.e. the language in the file name) from English. Not all talks are translated into all languages. The orginial English talks are in `../Data/ted-talks/XML_releases/xml/ted_en-20160408.xml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%ls ../Data/ted-talks/XML_releases/xml/*.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.) Understanding the xml structure and finding translations of the same text**\n",
    "\n",
    "To extract a translation of a particular text, we have to look into the xml structure of a single file. Open the xml file with the translations into Dutch in an editor (e.g. atom). \n",
    "\n",
    "\n",
    "Try to get a global idea of the structure. The following questions can help to guide you:\n",
    "\n",
    "(1) How many different talks does the file contain? (Hint: Scroll all the way down.)\n",
    "\n",
    "(2) Which tags indicate new talks? \n",
    "\n",
    "(3) Where do you find meta-information about a talk? Where do you find the translated text of the talk? Where do you find the transcription of the video? \n",
    "\n",
    "(4) Where do you find the identifier of the talk? Hint: look for a tag called 'talkid'. You can use this information to match translations with original talks. \n",
    "\n",
    "Tip: load one file and explore the information given about one talk using lxml.etree in python. You can use the code below to get started.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree as et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = '../Data/ted-talks/XML_releases/xml/ted_nl-20160408.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = et.parse(test)\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'language': 'nl'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The language is again provided in an attribute of the root:\n",
    "print(root.attrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the first layer of xml tags:\n",
    "for ch in root.getchildren():\n",
    "    print(ch.tag)\n",
    "    # Tip: Is there more information you can access at this point? Tip: explore text and attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1927\n"
     ]
    }
   ],
   "source": [
    "# Explore a single talk\n",
    "\n",
    "## First extract all talks (hint: each file capture one talk)\n",
    "\n",
    "talks = root.findall('file')\n",
    "print(len(talks))\n",
    "\n",
    "## Pick one talk to explore\n",
    "test_talk = talks[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head\n",
      "content\n"
     ]
    }
   ],
   "source": [
    "for ch in test_talk.getchildren():\n",
    "    print(ch.tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url\n",
      "pagesize\n",
      "dtime\n",
      "encoding\n",
      "content-type\n",
      "keywords\n",
      "speaker\n",
      "talkid\n",
      "videourl\n",
      "videopath\n",
      "date\n",
      "title\n",
      "description\n",
      "transcription\n",
      "translators\n",
      "reviewers\n",
      "wordnum\n",
      "charnum\n"
     ]
    }
   ],
   "source": [
    "# explore the meta information:\n",
    "\n",
    "head = test_talk.find('head')\n",
    "\n",
    "for ch in head.getchildren():\n",
    "    print(ch.tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "In this assignment, you will write code to explore the dataset. For instance, you will answer questions such as \n",
    "\n",
    "* How many talks are there in total? How many languages do the translations cover?\n",
    "* What is the oldest/latest talk?\n",
    "* Which speaker is most widely translated?\n",
    "\n",
    "Don't worry if you don't know how to solve these questions just now. You will start by working on the English talks (i.e. a single xml file). This will give you a feeling for the xml structure. You will write a number of small functions to extract information and compare talks. You should then be able to reuse your functions to explore the translations (i.e. work on multiple xml files). \n",
    "\n",
    "## Part I: Analyze the original English talks\n",
    "\n",
    "This part of the assignment only requires you to analyze the content of the xml file containing the original talks (in English). \n",
    "\n",
    "Create a python scipt which give you the following information:\n",
    "\n",
    "* What is the longest talk, what is the shortes talk, what is the average word count? (id and title, numbers) (`find_length`)\n",
    "* Oldest and latest talk (id and title, dates) (`find_date`)\n",
    "* Is there a speaker with multiple talks? (function: `find_speaker`)\n",
    "* How many English talks are there in total? (No function required)\n",
    "\n",
    "Excluded for now - can be integrated again:\n",
    "\n",
    "* Which text has the most references to the speaker themselves, which text has the most references to the audience? (Hint: count instances of first person pronouns I, me, my, mine, myself, normalize by number of words in the talk. make sure to account for upper and lower case letters) (if and title) ('find_most_pronouns'). Take into account that the talks can vary in length. Therefore, calculate the propostion of pronouns (n pronouns/total word count).\n",
    "\n",
    "Each of these aspects should be covered by a single function. Below, we give you some instructions about what the functions should do:\n",
    "\n",
    "* find_length: \n",
    "    * input: list of all talk elements (positional), length (longest/shortest, keyword argument)\n",
    "    * output: title(s), id(s), mean word count\n",
    "* find_date:\n",
    "    * input: list of all talk elements (positional), time (latest/oldest, keyword argument)\n",
    "    * output: title(s), id(s)\n",
    "<!-- * find_most_pronouns: \n",
    "   input: list of all talk elements (positional), pronouns (list of pronouns, keyword argument)\n",
    "   output: title(s), id(s), normalized number of pronouns -->\n",
    "* find_speaker:\n",
    "    * input: list of talk elemenets (position)\n",
    "    * output: dict mapping speakers with more than one talk to their talks (tuple of talk title and id)\n",
    "\n",
    "\n",
    "The script should execute all functions. Please print the output and answers to the questions above. Please don't forget to print the total number of talks. \n",
    "\n",
    "The script should be called `ted_english_analysis.py` and execute all functions when called from the command line. Add print statements so the output can be interpreted. For example, you the script should print:  \n",
    "\n",
    "```\n",
    "The total number of English talks is: [total number]\n",
    "\n",
    "Talk length: \n",
    "Longest talk: [title] (id: [id])\n",
    "Shortest talk: [title] (id: [id])\n",
    "Mean word count: [mean word count]\n",
    "```\n",
    "\n",
    "The script should only contain the functions listed above. If you use helper functions (highly recommended, see tips below), please put them in a script called `utils.py` and import them in the script called `ted_english_analysis.py`. \n",
    "\n",
    "\n",
    "You will receive points for the print statements and the script. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommended helper functions**\n",
    "\n",
    "We highly recommend definining small helper functions that extract information from a single file. \n",
    "\n",
    "First think about the different pieces of information you will need:\n",
    "\n",
    "* talk id\n",
    "* date\n",
    "* title\n",
    "* speaker\n",
    "* word count\n",
    "\n",
    "In addition, you want to load your file as an xml tree, access the root and extract all talks (as a list of xml elements). Tip: Use one element in the list to develop and test your helper functions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example:\n",
    "\n",
    "def load_root(path):\n",
    "    tree = et.parse(path)\n",
    "    root = tree.getroot()\n",
    "    return root\n",
    "\n",
    "def get_talks(root):\n",
    "    talks = root.findall('file')\n",
    "    return talks\n",
    "\n",
    "path = '../Data/ted-talks/XML_releases/xml/ted_en-20160408.xml'\n",
    "root = load_root(path)\n",
    "talks = get_talks(root)\n",
    "\n",
    "print(len(talks))\n",
    "\n",
    "# This can be your test example\n",
    "test_talk = talks[3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Analyze the translations\n",
    "\n",
    "This part of the assignment requires you to compare information accross multiple files. \n",
    "\n",
    "* Which language has the most translations? Which language has the least translations?\n",
    "\n",
    "* Which talk(s) is (are) translated into most languages? Please provide the English title(s) and the talk ids. \n",
    "\n",
    "* BONUS (just for fun - no points): What is the word for 'applaus' in the languages represented in the corpus?\n",
    "\n",
    "\n",
    "Please create a script called `ted_translation_analysis.py`. The script should print answers to the questions above. You can reuse (import, copy, modify) functions you created for Part I. \n",
    "\n",
    "Below, you will find several steps that guide you through the assignment. Code following these steps will earn you points (even if you do not manage to get the correct final output)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Map languages to filepaths (x points)\n",
    "\n",
    "Answering the questions above will require you to load and analyze xml files containing the translations of the English talks. It will be useful to have a dictionary that maps languages to filepaths. For example, the dictionary should contain the following entries: \n",
    "\n",
    "```\n",
    "{\n",
    "    'nl' :  '../Data/ted-talks/XML_releases/xml/ted_nl-20160408.xml',\n",
    "    'it' : '../Data/ted-talks/XML_releases/xml/ted_it-20160408.xml',\n",
    "    'fr-ca': '../Data/ted-talks/XML_releases/xml/ted_fr-ca-20160408.xml',\n",
    "    ...\n",
    "}\n",
    "```\n",
    "You can use the os or glob package to get a list of all filepaths in the directory `../Data/ted-talks/XML_releases/xml/`. Note that the language is provided between `ted_` and the release information (`-20160408`). Use string manipulation to access the language information. Attention: Some languages contain a hyphen (e.g. Canadian French fr-ca). \n",
    "\n",
    "In this assignment, you do not have to spell out the languages. It is alright if you provide the shortened names as they appear in the filepaths. \n",
    "\n",
    "Write a function (called `map_languages_to_paths`) that returns the dictionary. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fr-ca'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example string manipulation (feel free to choose a different strategy)\n",
    "lang_id = 'fr-ca-20160408'\n",
    "rev = lang_id[::-1]\n",
    "# split only at first point:\n",
    "f_id, lang = rev.split('-', 1)\n",
    "lang[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Write a function which returns the language with the most/least translations\n",
    "\n",
    "Name: `find_coverage`\n",
    "\n",
    "Input:\n",
    "\n",
    "* dictionary mapping languages to paths (positional)\n",
    "* most/least languages (e.g. 'most', 'least') (positional) \n",
    "\n",
    "Output: \n",
    "\n",
    "* language(s), number of tranlated talks\n",
    "\n",
    "\n",
    "Tip: You can simply check the number of talks in each xml file corresponding to a language. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Map talk ids to titles\n",
    "\n",
    "You can use talk ids to map English talks to their translations in other languages. Most of your code will work with these ids. In the end, you should map talk ids to titles. \n",
    "\n",
    "Write a function called `get_id_title_dict` that maps talk ids to English titles. Your function should take the path to the English file as input and return a dictionary (keys: talk ids, values: English talk titles). \n",
    "\n",
    "Tip: Reuse functions from the previous assignment. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 Map talks to languages they have been translated into\n",
    "\n",
    "Function name: Map_talks_to_languages\n",
    "\n",
    "Input: language filepath dict (result of Step 1)\n",
    "\n",
    "Output: a dictionary mapping talk ids to languages with translations of the talk. The dictionary should have the following structure (the example is made up):\n",
    "\n",
    "```\n",
    "{\n",
    "    '10': ['hy', 'nl', 'de', 'fr-ca']\n",
    "    '20': ['pl', 'da', 'nl', 'oc', 'ar']\n",
    "\n",
    "}\n",
    "\n",
    "```\n",
    "    \n",
    "    \n",
    "Tip: You can use defaultdict for this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Map number of languages to talks\n",
    "\n",
    "Goal: You want to know which talks haven been translated into how many languages. In the next step, you will want to rank talks by how many languages they've been translated into (or directly get the highest or lowest number of translations). To do this, it is useful to have a mapping from the number of translations to the talk ids\n",
    "\n",
    "Function name: map_nlang_to_talks\n",
    "\n",
    "Input: dictionary mapping talk ids to languages (list)\n",
    "\n",
    "Output: dictionary mapping number of translations (int) to talks (list of talk ids) having the following structure (this is not the correct output - just an example of the structure):\n",
    "\n",
    "```\n",
    "{\n",
    "\n",
    "    30 : ['200', '10', '31']\n",
    "    29 : ['201', '9', '7']\n",
    "    47 : ['1', '14', '209', '5']\n",
    "\n",
    "}\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Put it all together\n",
    "\n",
    "Put the functions you wrote above together to find the talk(s) that has (have) been translated into the most or least languages\n",
    "\n",
    "Name: find_top_coverage\n",
    "\n",
    "Input: \n",
    "    * dict mapping languages to filepaths (created in Step 1)\n",
    "    * most or least translations ('most' or 'least')\n",
    "    \n",
    "Output: A dictionary mapping the most/least talk titles to the languages they have been translated into. For example (this is not the correct solution - we just use it to show the structure):\n",
    "\n",
    "```\n",
    "{\n",
    "    'Dan Gross: Why gun violence can't be our new normal' : ['de', 'nl', 'it']\n",
    "    'Ang√©lica Dass: The beauty of human skin in every color': ['fr-ca', 'de', 'ceb']\n",
    "\n",
    "\n",
    "}\n",
    "```\n",
    "\n",
    "Use the output to print the correct solution to the terminal. (Tip: Call the same function twice - once for the most translations, once for the least translations)\n",
    "    \n",
    "    \n",
    "**Reminder:**\n",
    "\n",
    "Your functions should be called in the script called `ted_translation_analysis.py`. Please add print statements so the output can be interpreted. \n",
    "\n",
    "You will receive points for the print statements and the script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
