{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Resit - Part B\n",
    "\n",
    "\n",
    "Deadline: Tuesday, November 30 2021 before 17:00\n",
    "\n",
    "This part of the assignment should be submitted as a zip file containing two python modules:\n",
    "\n",
    "* utils.py\n",
    "* texts_to_coll.py\n",
    "* ASSIGNMENT-RESIT-A.ipynb (notebook containing part A)\n",
    "\n",
    "Please name your zip file as follows: RESIT-ASSIGNMENT.zip and upload it via Canvas (Resit Assignment). \n",
    "\n",
    "\n",
    "Please submit your assignment on Canvas: Resit Assignment\n",
    "\n",
    "If you have questions about this topic, please contact the teachers' mailing list: cltl.python.course@gmail.com.\n",
    "\n",
    "Note that we currently only check this mailing list once a day. We have given a week extra time, so please start timely.\n",
    "Answers to general questions will be covered on Piazza (https://piazza.com/class/kt1o9ir48ph50c), so please check if your question has already been answered.\n",
    "\n",
    "All of the covered chapters are important to this assignment. However, please pay special attention to:\n",
    "\n",
    "* Chapter 14 - Reading and writing text files\n",
    "* Chapter 15 - Off to analyzing text\n",
    "* Chapter 16 - Data formats I (CSV and TSV)\n",
    "* Chapter 19 - More about Natural Language Processing Tools (spaCy)\n",
    "\n",
    "\n",
    "\n",
    "In this assignment, we are going to write code which conversts raw text to a structured format frequently used in Natural Lanugage Processing. No matter what field you will end up working in, you will always have to be able to convert data from format A to format B. You have already gained some experience with such conversions in Block 4. \n",
    "\n",
    "**The CoNLL format**\n",
    "\n",
    "Before you use the output of a text analysis system, you usually want to store the output in a structured format. One way of doing this is to use naf - a format using xml. In this assignment, we are going to look at CoNLL, which is a table-based format (i.e. it is similar to csv/tsv). \n",
    "\n",
    "The format we are converting to is called CoNLL. CoNLL is the name of a conference (Conference on Natural Language Learning). Every year, the conference hosts a 'competion'. In this competition, participants have to build systems for a certain Natural Language Processing problem (usually referred to as 'task'). To compare results, participants have to stick to the CoNLL format. The format has become a popular format for storing the output of NLP systems. \n",
    "\n",
    "The goal of this assignment is to write a python module which processes all texts in ../Data/Dreams/. The output should be written to a new directory, in which each text is stored as a csv/tsv file following CoNLL conventions. \n",
    "\n",
    "**Text analysis with SpaCy**\n",
    "\n",
    "In part A of this assignment, you have already used SpaCy to process text. In this part of the assignment, you can make use of the code you have already written. The output files will contain the following information:\n",
    "\n",
    "* The tokens in each text\n",
    "* Information about the sentences in each text\n",
    "* Part-of-speech tags for each token\n",
    "* The lemma of each token\n",
    "* information about entities in a text (i.e. people, places, organizations, etc that are mentioned)\n",
    "\n",
    "**The assignment**\n",
    "\n",
    "We will guide you towards the final file-conversion step-by-step. The assignment is divided in 3 parts. We provide small toy exampls you can use to develop your code. As a final step, you will be asked to transfer all your code to python modules and process a directory of text files with it. \n",
    "\n",
    "Exercise 1: A guided tour of the CoNLL format\n",
    "\n",
    "Exercise 2: Writing a conversion function (text_to_conll)\n",
    "\n",
    "Exercise 3: Processing multiple files using python modules\n",
    "\n",
    "**Attention: This notebook should be placed in the same folder as the other Assignments!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding the CoNLL format\n",
    "\n",
    "\n",
    "The CoNLL format represents information about a text in table format. Each token is represented on a line. Each column contains a piece of information. Sentence-boundaries are marked by empty lines. In addition, each token has an index. This index starts with 1 and indentifies the positoion of the token in the sentence. Punctuation marks are also included. \n",
    "\n",
    "Consider the following example text: \n",
    "\n",
    "*This is an example text. The text mentions a former president of the United States, Barack Obama.*\n",
    "\n",
    "The representation of this sentence in CoNLL format looks like this:\n",
    "\n",
    "|   |      |   |      |        |  |\n",
    "|----|-----------|-----|-----------|--------|---|\n",
    "| 1  | This      | DT  | this      |        | O |\n",
    "| 2  | is        | VBZ | be        |        | O |\n",
    "| 3  | an        | DT  | an        |        | O |\n",
    "| 4  | example   | NN  | example   |        | O |\n",
    "| 5  | text      | NN  | text      |        | O |\n",
    "| 6  | .         | .   | .         |        | O |\n",
    "|    |           |     |           |        |   |\n",
    "| 1  | The       | DT  | the       |        | O |\n",
    "| 2  | text      | NN  | text      |        | O |\n",
    "| 3  | mentions  | VBZ | mention   |        | O |\n",
    "| 4  | a         | DT  | a         |        | O |\n",
    "| 5  | former    | JJ  | former    |        | O |\n",
    "| 6  | president | NN  | president |        | O |\n",
    "| 7  | of        | IN  | of        |        | O |\n",
    "| 8  | the       | DT  | the       | GPE    | B |\n",
    "| 9  | United    | NNP | United    | GPE    | I |\n",
    "| 10 | States    | NNP | States    | GPE    | I |\n",
    "| 11 | ,         | ,   | ,         |        | O |\n",
    "| 12 | Barack    | NNP | Barack    | PERSON | B |\n",
    "| 13 | Obama     | NNP | Obama     | PERSON | I |\n",
    "| 14 | .         | .   | .         |        | O |\n",
    "\n",
    "**The columns represent the following information:**\n",
    "\n",
    "* Column 1: Token index in sentence \n",
    "* Column 2: The token as it appears in the text (including punctuation)\n",
    "* Column 3: The part-of-speech tag\n",
    "* Column 4: The lemma of the token \n",
    "\n",
    "Column 5: Information about the type of entity (if the token is part of an expression referring to an entity). For example, Barack Obama is recognized as a person\n",
    "\n",
    "Column 6: Information about the position of the token in the entiy-mention. B stands for 'beginning', I stands for 'inside' and O stands for 'outside'. Anything that is not part of an entity mention is marked as 'outside'. (This is important information for dealing with entity mentions. Don't worry, you do not have to make use of this information here.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Writing the conversion function\n",
    "\n",
    "In this section of the assignment, we will guide you through writing your function. You can accomplish the entire conversion in a single function (i.e. there will be no helper functions at this point). We will first describe what your function should do and then provide small toy examples to help you with some of the steps. \n",
    "\n",
    "**The conversion function: text_to_conll**\n",
    "\n",
    "(1) Define a function called text_to_conll\n",
    "\n",
    "(2) The function should have the following parameters:\n",
    "\n",
    "* text: The input text (str) that should be processed and written to a conll file\n",
    "* nlp: the SpaCy model \n",
    "* output_dir: the directory the file should be written to\n",
    "* basename: the name of the output file without the path (i.e the file will be written to output_dir/basename\n",
    "* delimiter: the field delimiter (by default, it should be a tab)\n",
    "* start_with_index: By default, this should be True. \n",
    "* overwrite_existing_conll_file: By default, this should be set to True. \n",
    "\n",
    "(3) The function should do the following:\n",
    "\n",
    "* Convert text to CoNll format as shown in the example in exercise 1. \n",
    "\n",
    "* The file should have the following columns:\n",
    "    * Token index in sentence (as shown in example) If start_with_index is set to False, the first column should be the token.\n",
    "    * token \n",
    "    * part of speech tag (see tips below)\n",
    "    * lemma \n",
    "    * entity type (see tips below)\n",
    "    * entity iob label (indicates the position of a token in an entity-expression (see tips below)\n",
    "\n",
    "\n",
    "* If the parameter overwrite_existing_conll_file is set to True, the file should be written to output_dir/basename.\n",
    "\n",
    "* If the parameter overwrite_existing_conll_file is set to False, the function should check whether the file (path: output_dir/basename) exists. If it does, it should print 'File exists. Set param overwrite_exisiting_conll_file to True if you want to overwrite it.' If it does not exist, it should write it to the specified file. (See tips below)\n",
    "\n",
    "* The delimiter between fields should be the delimiter specified by the parameter delimiter.\n",
    "\n",
    "\n",
    "You can define the function in the notebook. Please test it using the following test text. Make sure to test the different paprameters. Your test file should be written to `test_dir/test_text.tsv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your function\n",
    "text = 'This is an example text. The text mentions a former president of the United States, Barack Obama.'\n",
    "basename = 'test_text.tsv'\n",
    "output_dir = 'test_dir'\n",
    "text_to_conll_simple(text, \n",
    "                         nlp, \n",
    "                         output_dir,\n",
    "                         basename,\n",
    "                         start_with_index = False,\n",
    "                        overwrite_existing_conll_file = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tip 0: Import spacy and load your model\n",
    "\n",
    "(See part A and chapter on SpaCy for more information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tip 1: Tokens, POS tags, and lemmas\n",
    "\n",
    "Experiment with a small example to get the tokens and pos tags. Please refer to the chapter on SpaCy for an example on how to process text with spacy.\n",
    "\n",
    "Spacy has different pos tags. For this exercise, it does not matter which one you use. Hint: To get a string (rather than a number, use the SpaCy attributes ending with '_'). \n",
    "\n",
    "You can use the code below to experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test = 'This is a test.'\n",
    "\n",
    "doc = nlp(test)\n",
    "\n",
    "tok = doc[0]\n",
    "tok.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tip 2: Entities\n",
    "\n",
    "**Entity types**\n",
    "\n",
    "Entities are things (usually people/places/organizations/etc) that exist in the real world. SpaCy can tag texts with entity types. If an expression refers to an entity in the world, it will receive a lable indicating the type (for example, Barack Obama will be tagged as 'PERSON'. Since the expression 'Barack Obama' consists of two tokens, each token will receive such a label. Use dir() on a token object to find out how to get this information. Hint: **Everything about entities starts with ('ent_')**\n",
    "\n",
    "**Position of the entity token**\n",
    "\n",
    "An expression referring to an entity can consist of multiple tokens. To indicate that multiple tokens are part of the same/of different expressions, we often use the IOB system. In this system, we indicate whether a token is outside an entity mention, inside an entity mention or at the beginning of an entity mention.  In practice, most tokens of a text will thus be tagged as 'O'. 'Barack' will be tagged as 'B' and 'Obama' as 'I' (see example above). SpaCy can do this type of labeling. Use dir() on a token object to find out how to get this information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_extension',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'string',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = 'This is a test.'\n",
    "\n",
    "doc = nlp(test)\n",
    "\n",
    "tok = doc[0]\n",
    "tok.text\n",
    "dir(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tip 3: Dealing with directories and files\n",
    "\n",
    "Use os to check if files or directories exist. You can also use os to make a directory if it does not exist yet.\n",
    "\n",
    "* os.path.isdir(path_to_dir) returns a boolean value. If the directory exists, it returns True. Else it returns False. You can use this to check if a directory exists. If it does not, you can make it.\n",
    "\n",
    "* os.path.isfile(path_to_file) returns a boolean value. If the file exists, it returns True. Else it returns False. \n",
    "\n",
    "* os.mkdir(path_to_dir) makes a new directory. Try it out and create a directory called 'test_dir' in the current directory. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: ../Data/books/Macbeth.txt\n",
      "File not found: ../Data/books/KingLear.txt\n"
     ]
    }
   ],
   "source": [
    "# Check if file exists\n",
    "\n",
    "import os\n",
    "\n",
    "a_path_to_a_file = '../Data/books/Macbeth.txt'\n",
    "\n",
    "if os.path.isfile(a_path_to_a_file):\n",
    "    print('File exists:', a_path_to_a_file)\n",
    "else:\n",
    "    print('File not found:', a_path_to_a_file)\n",
    "\n",
    "another_path_to_a_file = '../Data/books/KingLear.txt'\n",
    "\n",
    "if os.path.isfile(another_path_to_a_file):\n",
    "    print('File exists:', another_path_to_a_file)\n",
    "else:\n",
    "    print('File not found:', another_path_to_a_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory exists: ../Data/books/\n",
      "Directory not found: ../Data/films/\n"
     ]
    }
   ],
   "source": [
    "# check if directory exists\n",
    "\n",
    "a_path_to_a_dir = '../Data/books/'\n",
    "\n",
    "if os.path.isdir(a_path_to_a_dir):\n",
    "    print('Directory exists:', a_path_to_a_dir)\n",
    "else:\n",
    "    print('Directory not found:', a_path_to_a_dir)\n",
    "\n",
    "another_path_to_a_dir = '../Data/films/'\n",
    "\n",
    "if os.path.isdir(another_path_to_a_dir):\n",
    "    print('Directory exists:', another_path_to_a_dir)\n",
    "else:\n",
    "    print('Directory not found:', another_path_to_a_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building python modules to process files in a directory\n",
    "\n",
    "In this exercise, you will write two python modules:\n",
    "\n",
    "* utils.py\n",
    "* texts_to_conll.py\n",
    "\n",
    "The module texts_to_conll.py should do the following: \n",
    "\n",
    "* process all text files in a specified directory (we will use '../Data/Dreams')\n",
    "* write conll files representing these texts to another directory \n",
    "\n",
    "\n",
    "**Step 1: Preparation**:  \n",
    "\n",
    "* Create the two python modules in the same directory as this notebook \n",
    "* copy your function `text_to_conll` to the python module `texts_to_conll.py`\n",
    "* Move the function `load_text` you have defined in part A to `utils.py` and import it in `text_to_conll.py` \n",
    "* Move the function `get_paths` you have defined in part A to `utils.py` and import in it `text_to_conll.py`\n",
    "\n",
    "**Step 2: convert all text files in ../Data/dreams**:\n",
    "\n",
    "Use your functions to convert all files in  `../Data/dreams/`. Please fulfill the following criteria:\n",
    "\n",
    "* The new files should be placed in a directory placed in the current directory called dreams_conll/\n",
    "* Each file should be named as follows: [original name without extension].tsv (e.g.vicky1.tsv)\n",
    "* The files should contain an index column\n",
    "\n",
    "Tips:\n",
    "\n",
    "* Use a loop to iterate over the files in ../Data/dreams. \n",
    "* Use string methods and slicing to create the new filename from the original filename (e.g. split on '/' and/or '.', use indices to extract certain substrings, etc.)\n",
    "* Look at the resulting files to check if your code works. \n",
    "\n",
    "\n",
    "**Step 3: Test and submit**\n",
    "\n",
    "Please test your code carefully. Them submitt all your files in a .zip file via Canvas. \n",
    "\n",
    "\n",
    "**Congratulations! You have completed your first file conversion exercise!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IGNORE_ME!    vickie2.txt   vickie5.txt   vickie8.txt\n",
      "vickie1.txt   vickie3.txt   vickie6.txt   vickie9.txt\n",
      "vickie10.txt  vickie4.txt   vickie7.txt\n"
     ]
    }
   ],
   "source": [
    "# Files in '../Data/Dreams':\n",
    "%ls ../Data/Dreams/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
