{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization and Statistics\n",
    "\n",
    "At this point in the course, you have had some experience in getting and processing data, and exporting your results in a useful format. But after that stage, you also need to be able to *analyze* and *communicate* your results. Programming-wise, this is relatively easy. There are tons of great modules out there for doing statistics and making pretty graphs. The hard part is finding out what is the best way to communicate your findings.\n",
    "\n",
    "**At the end of this week, you will be able to:**\n",
    "- Perform exploratory data analysis, using both visual and statistical means.\n",
    "- Communicate your results using visualizations, that is:\n",
    "    - Make line plots.\n",
    "    - Make bar charts.\n",
    "    - Create maps.\n",
    "    - Create networks.\n",
    "\n",
    "**This requires that you already have (some) knowledge about:**\n",
    "- Loading and manipulating data.\n",
    "\n",
    "**If you want to learn more about these topics, you might find the following links useful:**\n",
    "- \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What kind of visualization to choose\n",
    "\n",
    "The following chart was made by ([Abela, 2006](http://extremepresentation.typepad.com/blog/2006/09/choosing_a_good.html)). It provides a first intuition on what kind of visualization to choose for your data. He also asks exactly the right question: **What do you want to show?** It is essential for any piece of communication to first consider: what is my main point? And after creating a visualization, to ask yourself: does this visualization indeed communicate what I want to communicate? (Ideally, also ask others: what kind of message am I conveying here?)\n",
    "\n",
    "![chart chooser](./images/chart_chooser.jpg)\n",
    "\n",
    "It's also apt to call this a 'Thought-starter'. There are many great kinds of visualizations that aren't in this diagram. To get some more inspiration, check out the example galleries for these libraries:\n",
    "\n",
    "* [D3.js](https://d3js.org/)\n",
    "* [Seaborn](https://seaborn.github.io/examples/index.html)\n",
    "* [Bokeh](http://bokeh.pydata.org/en/latest/docs/gallery.html)\n",
    "* [Pandas](http://pandas.pydata.org/pandas-docs/version/0.18.1/visualization.html)\n",
    "* [Matplotlib](http://matplotlib.org/gallery.html)\n",
    "* [Vis.js](http://visjs.org/index.html)\n",
    "\n",
    "But before you get carried away, do realize that **sometimes all you need is a good table**. Tables are visualizations, too! For a good guide on how to make tables, read the first three pages of [the LaTeX booktabs package documentation](http://ctan.cs.uu.nl/macros/latex/contrib/booktabs/booktabs.pdf). Also see [this guide](https://www.behance.net/gallery/Designing-Effective-Data-Tables/885004) with some practical tips.\n",
    "\n",
    "## What kind of visualizations *not* to choose\n",
    "\n",
    "As a warm-up exercise, take some time to browse [wtf-viz](http://viz.wtf/). For each of the examples, think about the following questions:\n",
    "\n",
    "1. What is the author trying to convey here?\n",
    "2. How did they try to achieve this?\n",
    "3. What went wrong?\n",
    "4. How could the visualization be improved? Or can you think of a better way to visualize this data?\n",
    "5. What is the take-home message here for you?\n",
    "\n",
    "For in-depth critiques of visualizations, see [Graphic Violence](https://graphicviolence.wordpress.com/). [Here](http://hanswisbrun.nl/tag/lieggrafiek/)'s a page in Dutch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A little history of visualization in Python\n",
    "\n",
    "As you've seen in the [State of the tools](https://www.youtube.com/watch?v=5GlNDD7qbP4) video, `Matplotlib` is one of the core libraries for visualization. It's feature-rich, and there are many tutorials and examples showing you how to make nice graphs. It's also fairly clunky, however, and the default settings don't make for very nice graphs. But because `Matplotlib` is so powerful, no one wanted to throw the library away. So now there are several modules that provide wrapper functions around `Matplotlib`, so as to make it easier to use and produce nice-looking graphs.\n",
    "\n",
    "* `Seaborn` is a visualization library that adds a lot of functionality and good-looking defaults to Matplotlib.\n",
    "* `Pandas` is a data analysis library that provides plotting methods for its `dataframe` objects.\n",
    "\n",
    "Behind the scenes, it's all still Matplotlib. So if you use any of these libraries to create a graph, and you want to customize the graph a little, it's usually a good idea to go through the `Matplotlib` documentation. Meanwhile, the developers of `Matplotlib` are still improving the library. If you have 20 minutes to spare, watch [this video](https://www.youtube.com/watch?v=xAoljeRJ3lU) on the new default colormap that will be used in Matplotlib 2.0. It's a nice talk that highlights the importance of color theory in creating visualizations.\n",
    "\n",
    "With the web becoming more and more popular, there are now also several libraries offering interactive visualizations using Javascript instead of Matplotlib. These are, among others:\n",
    "\n",
    "* [Bokeh](http://bokeh.pydata.org/en/latest/)\n",
    "* [NVD3](http://nvd3.org/)\n",
    "* [Lightning](http://lightning-viz.org/)\n",
    "* [MPLD3](http://mpld3.github.io/) (Also using Matplotlib)\n",
    "* [Plotly](https://plot.ly/)\n",
    "* [Vincent](https://vincent.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "\n",
    "Run the cell below. This will load relevant packages to use visualizations inside the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is special Jupyter notebook syntax, enabling interactive plotting mode.\n",
    "# In this mode, all plots are shown inside the notebook!\n",
    "# If you are not using notebooks (e.g. in a standalone script), don't include this.\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables\n",
    "\n",
    "There are (at least) two ways to output your data as a formatted table:\n",
    "\n",
    "* Using the `tabulate` package. (Use `pip install tabulate` to install it)\n",
    "* Using the `pandas` dataframe method `df.to_latex(...)`, `df.to_string(...)`, or even `df.to_clipboard(...)`.\n",
    "\n",
    "This is extremely useful if you're writing a paper. First version of the 'results' section: done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "table = [[\"spam\",42],[\"eggs\",451],[\"bacon\",0]]\n",
    "headers = [\"item\", \"qty\"]\n",
    "\n",
    "# Documentation: https://pypi.python.org/pypi/tabulate\n",
    "print(tabulate(table, headers, tablefmt=\"latex_booktabs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Documentation: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html\n",
    "df = pd.DataFrame(data=table, columns=headers)\n",
    "print(df.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've produced your LaTeX table, it's *almost* ready to put in your paper. If you're writing an NLP paper and your table contains scores for different system outputs, you might want to make the best scores **bold**, so that they stand out from the other numbers in the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More to explore\n",
    "\n",
    "The `pandas` library is *really* useful if you work with a lot of data (we'll also use it below). As Jake Vanderplas said in the [State of the tools](https://www.youtube.com/watch?v=5GlNDD7qbP4) video from Week 1, the `pandas` DataFrame is becoming the central format in the Python ecosystem. [Here](http://pandas.pydata.org/pandas-docs/stable/tutorials.html) is a page with `pandas` tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots\n",
    "\n",
    "This section shows you how to make plots using Matplotlib and Seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Even if you're not using Seaborn, this import and the next command change the Matplotlib defaults.\n",
    "# The effect of this is that Matplotlib plots look prettier!\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustrating Zipf's law\n",
    "\n",
    "We'll look at word frequencies to illustrate [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law): \"the frequency of any word is inversely proportional to its rank in the frequency table.\" Now what does that mean?\n",
    "\n",
    "For this illustration, we'll use the SUBTLEX-US frequency dataset, which is based on a huge collection of movie subtitles. One of the authors, Marc Brysbaert (Professor of Psychogy at the University of Ghent), notes that word frequencies in movie subtitles are the best approximation of the actual frequency distribution of the words that we hear every day. For this reason, these word frequencies are useful for psycholinguistic experiments.\n",
    "\n",
    "First we need to load the data. We'll use the CSV module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# We'll open the file using the DictReader class, which turns each row into a dictionary. \n",
    "# Keys in the dictionary are determined by the header of the file.\n",
    "\n",
    "entries = []\n",
    "with open('../Data/SUBTLEX-US/SUBTLEXus74286wordstextversion.txt') as f:\n",
    "    reader = csv.DictReader(f, delimiter='\\t')\n",
    "    for entry in reader:\n",
    "        # Turn the numbers into floats.\n",
    "        entry['SUBTLWF'] = float(entry['SUBTLWF'])\n",
    "        entry['Lg10WF'] = float(entry['Lg10WF'])\n",
    "        # And append the entry to the list.\n",
    "        entries.append(entry)\n",
    "\n",
    "# Sort the list of entries by frequency.\n",
    "entries = sorted(entries,\n",
    "                 key=lambda d:d['SUBTLWF'], # Sort by the word frequency\n",
    "                 reverse=True)              # Order the list from high to low "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll visualize the relationship between the frequency of the words and their rank, with the words ordered by their frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We use a list comprehension to get all the frequency values.\n",
    "frequencies = [e['SUBTLWF'] for e in entries]\n",
    "\n",
    "# Rank is just a list of numbers between 0 and the number of entries.\n",
    "ranks = list(range(len(entries)))\n",
    "\n",
    "# Plot the relationship in a scatterplot.\n",
    "plt.plot(ranks, frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph looks nearly empty, but if you look really closely, you'll see a blue line along the X and Y axes. What's needed for us to see the relation, is a transformation using the `log` scale. This transformation makes exponential functions linear. (You don't need to know this for the exam!) After transforming the ranks and frequencies, the graph should (more or less) look like a straight line!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "# The CSV already has log word frequencies.\n",
    "log_frequencies = [e['Lg10WF'] for e in entries]\n",
    "\n",
    "# We'll take the log of the rank, starting at 1 (because the log function isn't defined for 0).\n",
    "log_rank = [log(i) for i in range(1,len(log_frequencies)+1)]\n",
    "\n",
    "# And plot the graph again. This should be a (more or less) straight line!\n",
    "plt.plot(log_rank, log_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "\n",
    "Let's look at correlation between values in Python. We'll explore two measures: Pearson and Spearman correlation. Given two lists of numbers, Pearson looks whether there is any *linear relation* between those numbers. This is contrasted by the Spearman measure, which aims to see whether there is any *monotonic relation*. The difference between linear and monotonic is that the latter is typically less strict:\n",
    "\n",
    "* Monotonic: a constant relation between two lists of numbers.\n",
    "    1. if a number in one list increases, so does the number in the other list, or \n",
    "    2. if a number in one list increases, the number in the other list decreases.\n",
    "* Linear: similar to monotonic, but the increase or decrease can be modeled by a straight line.\n",
    "\n",
    "Here is a small example to illustrate the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Scipy offers many statistical functions, among which the Pearson and Spearman correlation measures.\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# X is equal to [1,2,3,...,99,100]\n",
    "x = list(range(100))\n",
    "\n",
    "# Y is equal to [1^2, 2^2, 3^2, ..., 99^2, 100^2]\n",
    "y = [i**2 for i in x]\n",
    "\n",
    "# Z is equal to [100,200,300, ..., 9900, 10000]\n",
    "z = [i*100 for i in x]\n",
    "\n",
    "# Plot x and y.\n",
    "plt.plot(x, y, label=\"X and Y\")\n",
    "\n",
    "# Plot y and z in the same plot.\n",
    "plt.plot(x, z, label=\"X and Z\")\n",
    "\n",
    "# Add a legend.\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correlation, significance = pearsonr(x,y)\n",
    "print('The Pearson correlation between X and Y is:', correlation)\n",
    "\n",
    "correlation, significance = spearmanr(x,y)\n",
    "print('The Spearman correlation between X and Y is:', correlation)\n",
    "\n",
    "print('----------------------------------------------------------')\n",
    "\n",
    "correlation, significance = pearsonr(x,z)\n",
    "print('The Pearson correlation between X and Z is:', correlation)\n",
    "\n",
    "correlation, significance = spearmanr(x,z)\n",
    "print('The Spearman correlation between X and Z is:', correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spearman correlation is perfect in both cases, because with each increase in X, there is an increase in Y. But because that increase isn't the same at each step, we see that the Pearson correlation is slightly lower.\n",
    "\n",
    "In Natural Language Processing, people typically use the Spearman correlation because they are interested in *relative scores*: does the model score A higher than B? The exact score often doesn't matter. Hence Spearman provides a better measure, because it doesn't penalize models for non-linear behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory visualization\n",
    "\n",
    "Before you start working on a particular dataset, it's often a good idea to explore the data first. If you have text data; open the file and see what it looks like. If you have numeric data, it's a good idea to visualize what's going on. This section shows you some ways to do exactly that. We'll work with another data file by Brysbaert and colleagues, consisting of concreteness ratings. I.e. how abstract or concrete participants think a given word is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's load the data first.\n",
    "concreteness_entries = []\n",
    "with open('../Data/concreteness/Concreteness_ratings_Brysbaert_et_al_BRM.txt') as f:\n",
    "    reader = csv.DictReader(f, delimiter='\\t')\n",
    "    for entry in reader:\n",
    "        entry['Conc.M'] = float(entry['Conc.M'])\n",
    "        concreteness_entries.append(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any kind of ratings, you can typically expect the data to have a normal-ish distribution: most of the data in the middle, and increasingly fewer scores on the extreme ends of the scale. We can check whether the data matches our expectation using a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "for entry in concreteness_entries:\n",
    "    scores.append(entry['Conc.M'])\n",
    "\n",
    "# Plot the distribution of the scores.\n",
    "sns.distplot(scores, kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "Surprise! It doesn't. This is a typical *bimodal* distribution with two peaks. Going back to [the original article](http://link.springer.com/sharelink/10.3758/s13428-013-0403-5), this is also mentioned in their discussion:\n",
    "\n",
    "> One concern, for instance, is that concreteness and abstractness may be not the two extremes of a quantitative continuum (reflecting the degree of sensory involvement, the degree to which words meanings are experience based, or the degree of contextual availability), but two qualitatively different characteristics. One argument for this view is that the distribution of concreteness ratings is bimodal, with separate peaks for concrete and abstract words, whereas ratings on a single, quantitative dimension usually are unimodal, with the majority of observations in the middle (Della Rosa et al., 2010; Ghio, Vaghi, & Tettamanti, 2013)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare, here are sentiment scores for English (from [Dodds et al. 2014](http://www.uvm.edu/storylab/share/papers/dodds2014a/)), where native speakers rated a list of 10,022 words on a scale from 0 (negative) to 9 (positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the data (one score per line, words are in a separate file).\n",
    "with open('../Data/Dodds2014/data/labMTscores-english.csv') as f:\n",
    "    scores = [float(line.strip()) for line in f]\n",
    "\n",
    "# Plot the histogram\n",
    "sns.distplot(scores, kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because Dodds et al. collected data from several languages, we can plot the distributions for multiple languages and see whether they all have normally distributed scores. We will do this with a [Kernal Density Estimation](https://en.wikipedia.org/wiki/Kernel_density_estimation) plot. Basically, such a plot shows you the probability distribution (the chance of getting a particular score) as a continuous line. Because it's a line rather than a set of bars, you can show many of them in the same graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is necessary because the kdeplot function only accepts arrays.\n",
    "import numpy as np\n",
    "\n",
    "# This is necessary to get all the separate files.\n",
    "import glob\n",
    "\n",
    "# Get all the score files.\n",
    "filenames = glob.glob('../Data/Dodds2014/data/labMTscores-*.csv')\n",
    "\n",
    "# Showing the first 5, because else you can't keep track of all the lines.\n",
    "for filename in filenames[:5]:\n",
    "    # Read the language from the filename\n",
    "    language = filename.split('-')[1]\n",
    "    language = language.split('.')[0]\n",
    "    with open(filename) as f:\n",
    "        scores = [float(line.strip()) for line in f]\n",
    "        sns.kdeplot(np.array(scores), label=language)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at all those unimodal distributions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Continuing with the concreteness dataset**\n",
    "\n",
    "It is commonly known in the literature on concreteness that concreteness ratings are (negatively) correlated with word length: the longer a word, the more abstract it typically is. Let's try to visualize this relation. We can plot the data using a regression plot to verify this. In addition, we're using a Pandas DataFrame to plot the data. You could also just use `sns.regplot(word_length, rating, x_jitter=0.4)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create two lists of scores to correlate.\n",
    "word_length = []\n",
    "rating = []\n",
    "for entry in concreteness_entries:\n",
    "    word_length.append(len(entry['Word']))\n",
    "    rating.append(entry['Conc.M'])\n",
    "\n",
    "# Create a Pandas Dataframe. \n",
    "# I am using this here, because Seaborn adds text to the axes if you use DataFrames.\n",
    "# You could also use pd.read_csv(filename,delimiter='\\t') if you have a file ready to plot.\n",
    "df = pd.DataFrame.from_dict({\"Word length\": word_length, \"Rating\": rating})\n",
    "\n",
    "# Plot a regression line and (by default) the scatterplot. \n",
    "# We're adding some jitter because all the points fall on one line. \n",
    "# This makes it difficult to see how densely 'populated' the area is.\n",
    "# But with some random noise added to the scatterplot, you can see more clearly \n",
    "# where there are many dots and where there are fewer dots.\n",
    "sns.regplot('Word length', 'Rating', data=df, x_jitter=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't look like a super strong correlation. We can check by using the correlation measures from SciPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If we're interested in predicting the actual rating.\n",
    "corr, sig = pearsonr(word_length, rating)\n",
    "print('Correlation, according to Pearsonr:', corr)\n",
    "\n",
    "# If we're interested in ranking the words by their concreteness.\n",
    "corr, sig = spearmanr(word_length, rating)\n",
    "print('Correlation, according to Spearmanr:', corr)\n",
    "\n",
    "# Because word length is bound to result in ties (many words have the same length), \n",
    "# some people argue you should use Kendall's Tau instead of Spearman's R:\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "corr, sig = kendalltau(word_length, rating)\n",
    "print(\"Correlation, according to Kendall's Tau:\", corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you've seen several different plots, hopefully the general pattern is becoming clear: visualization typically consists of three steps:\n",
    "\n",
    "1. Load the data.\n",
    "2. Organize the data in such a way that you can feed it to the visualization function.\n",
    "3. Plot the data using the function of your choice.\n",
    "\n",
    "There's also an optional **fourth step**: After plotting the data, tweak the plot until you're satisfied. Of these steps, the second and fourth are usually the most involved. Now let's try a slightly more difficult graph: **the bar plot**. The following example shows you how to draw a bar plot and customize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We want to visualize how far I've walked this week (using some random numbers).\n",
    "# Here's a dictionary that can be loaded as a pandas dataframe. Each item corresponds to a COLUMN.\n",
    "distance_walked = {'days': ['Monday','Tuesday','Wednesday','Thursday','Friday'],\n",
    "                   'km': [5,6,5,19,4]}\n",
    "\n",
    "# Turn it into a dataframe.\n",
    "df = pd.DataFrame.from_dict(distance_walked)\n",
    "\n",
    "# Plot the data using seaborn's built-in barplot function.\n",
    "# To select the color, I used the color chart from here: \n",
    "# http://stackoverflow.com/questions/22408237/named-colors-in-matplotlib\n",
    "ax = sns.barplot(x='days',y='km',color='lightsteelblue',data=df)\n",
    "\n",
    "# Here's a first customization. \n",
    "# Using the Matplotlib object returned by the plotting function, we can change the X- and Y-labels.\n",
    "ax.set_ylabel('km')\n",
    "ax.set_xlabel('')\n",
    "\n",
    "# Each matplotlib object consists of lines and patches that you can modify.\n",
    "# Each bar is a rectangle that you can access through the list of patches.\n",
    "# To make Thursday stand out even more, I changed its face color.\n",
    "ax.patches[3].set_facecolor('palevioletred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You can also plot a similar chart using Pandas.\n",
    "ax = df.plot(x='days',y='km',kind='barh') # or kind='bar'\n",
    "\n",
    "# Remove the Y label.\n",
    "ax.set_ylabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### On your own\n",
    "\n",
    "We'll work with data from Donald Trump's Facebook page. The relevant file is `Data/Trump-Facebook/FacebookStatuses.tsv`. Try to create a visualization that answers one of the following questions:\n",
    "\n",
    "1. How does the number of responses to Trump's posts change over time?\n",
    "2. What webpages does Donald Trump link to, and does this change over time? Which is the most popular? Are there any recent newcomers?\n",
    "3. What entities does Trump talk about?\n",
    "4. Starting March 2016 (when the emotional responses were introduced on Facebook), how have the emotional responses to Trumps messages developed?\n",
    "5. [Question of your own.]\n",
    "\n",
    "Try to at least think about what kind of visualization might be suitable to answer these questions, and we'll discuss this question in class on Monday. More specific questions:\n",
    "\n",
    "* What kind of preprocessing is necessary before you can start visualizing the data?\n",
    "* What kind of visualization is suitable for answering these questions?\n",
    "    - What sort of chart would you choose?\n",
    "    - How could you use color to improve your visualization?\n",
    "* What might be difficult about visualizing this data? How could you overcome those difficulties?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open the data.\n",
    "\n",
    "\n",
    "# Process the data so that it can be visualized.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the data.\n",
    "\n",
    "\n",
    "# Modify the plot.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maps\n",
    "\n",
    "Maps are a *huge* subject that we won't cover in detail. We'll only discuss a very simple use case: suppose you have some locations that you want to show on a map. How do you do that?\n",
    "\n",
    "First we need to import the relevant library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll use the Basemap module.\n",
    "from mpl_toolkits.basemap import Basemap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create a `Basemap` instance. This instance contains all the data that is necessary to draw the area you're interested in. You can create a `Basemap` instance by calling the `Basemap` class with 6 parameters:\n",
    "\n",
    "* The width of the map in meters. You can typically find a rough estimate online. Start from there, and find the optimal width by trial and error.\n",
    "* The height of the map in meters. You can find the optimal width through the same procedure.\n",
    "* The projection. Different projections are [listed here](http://matplotlib.org/basemap/users/mapsetup.html).\n",
    "* The resolution. How detailed you want the borders to be drawn. Possible values are `c` (crude), `l` (low), `i` (intermediate), `h` (high), `f` (full) and `None`. The more detailed the borders are, the slower the drawing process becomes. So during development, you should use a lower resolution so that you see the results more quickly.\n",
    "* Latitude at the center of the map. You can find this value (or a rough approximation) online.\n",
    "* Longitude at the center of the map.\n",
    "\n",
    "Using the `Basemap` object, you can draw the coastlines and the border lines, and then you have a nice map!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the map. This may take a while..\n",
    "m = Basemap(width=275000,height=360000,projection='lcc',\n",
    "            resolution='h',lat_0=52.25,lon_0=5.2)\n",
    "\n",
    "# Draw coastlines and borders.\n",
    "m.drawcoastlines(linewidth=1)\n",
    "m.drawcountries(linewidth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Basemap` [documentation](http://matplotlib.org/basemap/users/geography.html) also shows you how to draw more detailed maps. Here's one of their examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setup Lambert Conformal basemap.\n",
    "# set resolution=None to skip processing of boundary datasets.\n",
    "m = Basemap(width=12000000,height=9000000,projection='lcc',\n",
    "            resolution=None,lat_1=45.,lat_2=55,lat_0=50,lon_0=-107.)\n",
    "m.shadedrelief()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualization works very well for the USA, since it's such a large area. But since the Netherlands are much smaller, you end up with a very blurry map (because you need to zoom in so much). One option would be to add coastlines and borders again (play around with this by uncommenting the commands below). But for publications, I would probably use the first (black-and-white) map, because it's so clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = Basemap(width=275000,height=360000,projection='lcc',\n",
    "            resolution='h',lat_0=52.25,lon_0=5.2)\n",
    "m.shadedrelief()\n",
    "# m.drawcoastlines(linewidth=1,color='white')\n",
    "# m.drawcountries(linewidth=1,color='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermezzo: degrees, minutes, seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latitude and longitude are sometimes given in decimal degrees, and sometimes in degree-minute-second (DMS) notation. E.g. Amsterdam is located at 52°22′N 4°54′E. The first number corresponds to the latitude , while the second number corresponds to the longitude. This is how you convert between DMS and decimal degrees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decimal_degrees(degrees, minutes, seconds):\n",
    "    \"Convert DMS-formatted degrees to decimal degrees.\"\n",
    "    dd = degrees + (minutes/60) + (seconds/3600)\n",
    "    return dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting points and values on a map\n",
    "\n",
    "Now let's plot some points on the map! We'll use the plot function for this, but for collections of points you may want to use `m.scatter(...)`. See [this page](http://matplotlib.org/api/markers_api.html) for more instructions on how to control the way markers look.\n",
    "\n",
    "One of the most basic things you can do is put a dot on the map corresponding to the capital city. Here's how to do that for the Netherlands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Longitude and latitude for Amsterdam.\n",
    "lon, lat = 4.8979956033677, 52.374436\n",
    "\n",
    "# Draw the map again.\n",
    "m.drawcoastlines(linewidth=1)\n",
    "m.drawcountries(linewidth=1)\n",
    "\n",
    "# Plot Amsterdam on the map.\n",
    "# The latlon keyword tells Python that lon and lat are longitude and latitude values.\n",
    "# These are automatically converted to the right coordinates for the current map projection.\n",
    "# If you leave out the latlon keyword, you need to separately convert the coordinates, like so:\n",
    "# lon, lat = m(lon, lat)\n",
    "m.plot(lon, lat, 'ro', latlon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that looks good, but what if we wanted to plot *values* on the map? We can do that as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "value = '42'\n",
    "\n",
    "# Convert longitude and latitude to map coordinates.\n",
    "x,y = m(lon, lat)\n",
    "\n",
    "# Draw the map again.\n",
    "m.drawcoastlines(linewidth=1)\n",
    "m.drawcountries(linewidth=1)\n",
    "\n",
    "# Plot the value on the map. Note that we're using a Matplotlib function now!\n",
    "plt.text(x,y,value,weight='extra bold', color='red',size=14, va='center', ha='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, that doesn't look quite right. The coastline makes the number very hard to read. We can solve this by putting a white marker behind the number. (This is sort of cheating, but I found this trick to be very useful.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m.drawcoastlines(linewidth=1)\n",
    "m.drawcountries(linewidth=1)\n",
    "\n",
    "# Plot a marker.\n",
    "m.plot(x,y,'wo',mec='white',markersize=15)\n",
    "\n",
    "# Plot the value on the map.\n",
    "plt.text(x,y,value,weight='extra bold', color='red',size=14, va='center', ha='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much more readable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn\n",
    "\n",
    "Try to create a map for some other country than the USA or the Netherlands, and mark the 5 biggest cities on the map. You can use Google/Wikipedia or the `Geopy` module to get the latitudes and longitudes. Here's a reminder for the `geopy` module:\n",
    "\n",
    "```python\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "location = geolocator.geocode(place)\n",
    "lon,lat  = location.longitude, location.latitude\n",
    "```\n",
    "\n",
    "If you do use `geopy`, please don't forget to cache your results. Store coordinates in a dictionary or a JSON file, with key: placename, value: (longitude, latitude)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Here](http://maxberggren.se/2015/08/04/basemap/) is another tutorial for drawing a map using Basemap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More to explore\n",
    "\n",
    "Other libraries for visualizing data on a map are:\n",
    "\n",
    "* [Vincent](https://vincent.readthedocs.io/en/latest/)\n",
    "* [Folium](https://github.com/python-visualization/folium)\n",
    "* [Cartopy](http://scitools.org.uk/cartopy/docs/latest/)\n",
    "* [Geoplotlib](https://github.com/andrea-cuttone/geoplotlib)\n",
    "* [Kartograph](http://kartograph.org/)\n",
    "\n",
    "Beyond displaying points on a map, you might also want to create [choropleth maps](https://en.wikipedia.org/wiki/Choropleth_map). We won't cover this subject in detail, but for anything more detailed than countries (states, provinces, municipalities, etc), you typically need to have a **shapefile** (often in GeoJSON format) that tells the mapping library what the relevant regions are. In those shapefiles, regions are represented as polygons: complex shapes that can be overlaid on a map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks\n",
    "\n",
    "Some data is best visualized as a network. There are several options out there for doing this. The easiest is to use the NetworkX library and either plot the network using Matplotlib, or export it to JSON or GEXF (Graph EXchange Format) and visualize the network using external tools.\n",
    "\n",
    "Let's explore a bit of WordNet today. For this, we'll want to import the NetworkX library, as well as the WordNet module. We'll look at the first synset for *dog*: `dog.n.01`, and how it's positioned in the WordNet taxonomy. All credits for this idea go to [this blog](http://www.randomhacks.net/2009/12/29/visualizing-wordnet-relationships-as-graphs/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.util import bigrams # This is a useful function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Networks are made up out of *edges*: connections between *nodes* (also called *vertices*). To build a graph of the WordNet-taxonomy, we need to generate a set of edges. This is what the function below does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hypernym_edges(synset):\n",
    "    \"\"\"\n",
    "    Function that generates a set of edges \n",
    "    based on the path between the synset and entity.n.01\n",
    "    \"\"\"\n",
    "    edges = set()\n",
    "    for path in synset.hypernym_paths():\n",
    "        synset_names = [s.name() for s in path]\n",
    "        # bigrams turns a list of arbitrary length into tuples: [(0,1),(1,2),(2,3),...]\n",
    "        # edges.update adds novel edges to the set.\n",
    "        edges.update(bigrams(synset_names))\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the synset 'dog.n.01'\n",
    "dog = wn.synset('dog.n.01')\n",
    "\n",
    "# Generate a set of edges connecting the synset for 'dog' to the root node (entity.n.01)\n",
    "edges = hypernym_edges(dog)\n",
    "\n",
    "# Create a graph object.\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add all the edges that we generated earlier.\n",
    "G.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can actually start drawing the graph. We'll increase the figure size, and use the `draw_spring` method (that implements the Fruchterman-Reingold layout algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Increasing figure size for better display of the graph.\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 11, 11\n",
    "\n",
    "# Draw the actual graph.\n",
    "nx.draw_spring(G,with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is interesting about this is that there is a *cycle* in the graph! This is because *dog* has two hypernyms, and those hypernyms are both superseded (directly or indirectly) by *animal.n.01*.\n",
    "\n",
    "What is not so good is that the graph looks pretty ugly: there are several crossing edges, which is totally unnecessary. There are better layouts implemented in NetworkX, but they do require you to install `pygraphviz`. Once you've done that, you can execute the next cell. (And if not, then just assume it looks much prettier!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Install pygraphviz first: pip install pygraphviz\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "\n",
    "# Let's add 'cat' to the bunch as well.\n",
    "cat = wn.synset('cat.n.01')\n",
    "cat_edges = hypernym_edges(cat)\n",
    "G.add_edges_from(cat_edges)\n",
    "\n",
    "# Use the graphviz layout. First compute the node positions..\n",
    "positioning = graphviz_layout(G)\n",
    "\n",
    "# And then pass node positions to the drawing function.\n",
    "nx.draw_networkx(G,pos=positioning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Question**\n",
    "\n",
    "How do dogs differ from cats, according to WordNet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "Can you think of any data other than WordNet-synsets that could be visualized as a network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### More to explore\n",
    "\n",
    "* Python's network visualization tools are fairly limited (though I haven't really explored Pygraphviz (and Graphviz itself is able to create [examples like these](http://www.graphviz.org/Gallery.php))). It's usually easier to export the graph to GEXF and visualize it using [Gephi](https://gephi.org/) or [SigmaJS](http://sigmajs.org/). Gephi also features plugins, which enable you to create interactive visualizations. See [here](https://github.com/evanmiltenburg/dm-graphs/) for code and a link to a demo that I made.\n",
    "\n",
    "* For analyzing graphs, I like to use either Gephi, or the [python-louvain](http://perso.crans.org/aynaud/communities/) library, which enables you to cluster nodes in a network.\n",
    "\n",
    "* Some of the map-making libraries listed above also provide some cool functionality to create graphs on a map. This is nice to visualize e.g. relations between countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
